{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec113a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms, models\n",
    "import pretrainedmodels\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import random\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_into_lst(lst):\n",
    "    answer=dict()\n",
    "    for num in lst:\n",
    "        if num not in answer.keys():\n",
    "            answer[num]=1\n",
    "        else:\n",
    "            answer[num]+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_data_sequence(json_file):\n",
    "    random.shuffle(json_file)\n",
    "    indi_data = []\n",
    "    for i in range(len(json_file)): \n",
    "        for j in range(json_file[i]['file_num']):\n",
    "            json_dict = dict()\n",
    "            json_dict['file_dir'] = json_file[i]['file_dir'][j]\n",
    "            json_dict['class'] = json_file[i]['class number']\n",
    "            json_dict['instance_num'] = i \n",
    "            \n",
    "            indi_data += [json_dict] \n",
    "    return indi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatset(object):\n",
    "    \n",
    "    def __init__(self,all_json, transform):\n",
    "        self.all_json = all_json\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        imgs = self.transform(Image.open(self.all_json[idx]['file_dir']).convert(\"RGB\"))\n",
    "        targets = torch.from_numpy(np.array(self.all_json[idx]['class'])).type(torch.LongTensor)\n",
    "        instance = self.all_json[idx]['instance_num']\n",
    "        \n",
    "        return imgs ,targets, instance\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def save_config_file(model_checkpoints_folder, args):\n",
    "    if not os.path.exists(model_checkpoints_folder):\n",
    "        os.makedirs(model_checkpoints_folder)\n",
    "        with open(os.path.join(model_checkpoints_folder, 'config.yml'), 'w') as outfile:\n",
    "            yaml.dump(args, outfile, default_flow_style=False)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        \n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class SimCLR(object):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        numoflabel_1 = len(os.listdir(r'C:\\Users\\yonsei\\Desktop\\4 scar types\\Resize/Train/Adhesive'))\n",
    "        numoflabel_2 = len(os.listdir(r'C:\\Users\\yonsei\\Desktop\\4 scar types\\Resize/Train/Bulge'))\n",
    "        numoflabel_3 = len(os.listdir(r'C:\\Users\\yonsei\\Desktop\\4 scar types\\Resize/Train/Hypertrophy'))\n",
    "        numoflabel_4 = len(os.listdir(r'C:\\Users\\yonsei\\Desktop\\4 scar types\\Resize/Train/Linear'))\n",
    "        weights = torch.tensor([numoflabel_1,numoflabel_2,numoflabel_3,numoflabel_4], dtype=torch.float32)\n",
    "        weights = weights / weights.sum()\n",
    "        weights = 1.0 / weights\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        \n",
    "        self.args = kwargs['args']\n",
    "        self.model = kwargs['model'].cuda()\n",
    "        self.optimizer = kwargs['optimizer']\n",
    "        self.scheduler = kwargs['scheduler']\n",
    "        self.writer = SummaryWriter()\n",
    "        logging.basicConfig(filename=os.path.join(self.writer.log_dir, 'training.log'), level=logging.DEBUG)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(weight=weights.cuda()).cuda()\n",
    "\n",
    "    def instance_loss(self, features, targets, instances):\n",
    "        \n",
    "        targets = np.array(targets)\n",
    "        instances = np.array(instances)\n",
    "        loss_class = torch.zeros((3,self.args.num_class))\n",
    "        features = F.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "        \n",
    "        set_instances = list(set(instances))\n",
    "        instance_counts = count_into_lst(instances)\n",
    "        \n",
    "        for i in range(len(set_instances)):\n",
    "            if instance_counts[set_instances[i]] == 1:\n",
    "                pass\n",
    "            else:\n",
    "                idx = np.where(np.array(instances) == set_instances[i])\n",
    "                loss_class[0,targets[idx[0][0]]] = torch.mean(2-(similarity_matrix[idx[0][0]:idx[0][len(idx[0])-1]]+1))\n",
    "        set_targets = list(set(targets))\n",
    "        for i in (set_targets):\n",
    "            p_idx = np.where(targets == i)\n",
    "            n_idx = np.where(targets != i)\n",
    "            loss_class[1,i] = torch.mean(2-(similarity_matrix[p_idx[0]][:,p_idx[0]]+1))\n",
    "            loss_class[2,i] = torch.mean(1/(2-(similarity_matrix[p_idx[0]][:,n_idx[0]]+1)) - 0.5)\n",
    "            \n",
    "        instance_loss = self.args.instance_loss_alpha*torch.sum(loss_class[0,:]) + self.args.instance_loss_beta*torch.sum(loss_class[1,:]) + self.args.instance_loss_gamma*torch.sum(loss_class[2,:])\n",
    "        \n",
    "        return instance_loss\n",
    "\n",
    "    def train(self, json_Train,valid_loader,transforms):\n",
    "        scaler = GradScaler(enabled=self.args.fp16_precision)\n",
    "        \n",
    "        # save config file\n",
    "        save_config_file(self.writer.log_dir, self.args)\n",
    "            \n",
    "        n_iter = 0\n",
    "        logging.info(f\"Start SimCLR training for {self.args.epochs} epochs.\")\n",
    "        logging.info(f\"Training with gpu: {self.args.disable_cuda}.\")\n",
    "        \n",
    "        for epoch_counter in range(self.args.epochs):\n",
    "            phase = 'train'\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                self.model.train()\n",
    "\n",
    "                json_shuffle = instance_data_sequence(json_Train)\n",
    "\n",
    "                train_dataset = CustomDatset(all_json = json_shuffle, transform = transforms)\n",
    "\n",
    "                train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset, batch_size=self.args.batch_size, shuffle=False,\n",
    "                    num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "                print(epoch_counter+1)\n",
    "                for images,targets, instances in tqdm(train_loader):\n",
    "\n",
    "                    images = images.cuda()\n",
    "\n",
    "\n",
    "                    with autocast(enabled=self.args.fp16_precision):\n",
    "                        pred,features = self.model(images)\n",
    "                        loss1 = self.instance_loss(features,targets,instances)\n",
    "                        targets = targets.cuda()\n",
    "                        loss2 = self.criterion(pred, targets)\n",
    "                        loss = loss1 + loss2\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    if n_iter % self.args.log_every_n_steps == 0:\n",
    "                        self.writer.add_scalar('loss', loss, global_step=n_iter)\n",
    "                        self.writer.add_scalar('learning_rate', self.scheduler.get_lr()[0], global_step=n_iter)\n",
    "\n",
    "                    n_iter += 1\n",
    "\n",
    "                # warmup for the first 10 epochs\n",
    "                if epoch_counter >= 10:\n",
    "                    self.scheduler.step()\n",
    "                logging.debug(f\"Epoch: {epoch_counter}\\tinstance_Loss: {loss1}\\tentrophy_Loss: {loss2}\")\n",
    "\n",
    "            phase = 'valid'\n",
    "            self.model.eval()\n",
    "            valid_loss = 0\n",
    "            valid_idx = 0\n",
    "            with torch.no_grad():\n",
    "                for images,targets, instances in tqdm(valid_loader):\n",
    "                    valid_idx += 1\n",
    "                    images = images.cuda()\n",
    "                    pred,features = self.model(images)\n",
    "                    loss1 = self.instance_loss(features,targets,instances)\n",
    "                    targets = targets.cuda()\n",
    "                    loss2 = self.criterion(pred, targets)\n",
    "                    valid_loss +=  loss2\n",
    "                    \n",
    "                valid_loss = valid_loss/valid_idx\n",
    "                self.writer.add_scalar('Val_loss', valid_loss, global_step=n_iter)\n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "        logging.info(\"Training has finished.\")\n",
    "        checkpoint_name = 'checkpoint_{:04d}.pth.tar'.format(self.args.epochs)\n",
    "        save_checkpoint({\n",
    "            'epoch': self.args.epochs,\n",
    "            'arch': self.args.arch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }, is_best=False, filename=os.path.join(self.writer.log_dir, checkpoint_name))\n",
    "        logging.info(f\"Model checkpoint and metadata has been saved at {self.writer.log_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43453442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ResNetSimCLR(nn.Module):\n",
    "\n",
    "    def __init__(self, base_model, out_dim, num_class):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        self.resnet_dict = {\n",
    "                            \"resnet50\": models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2),\n",
    "                           \"resnet152\": models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)}\n",
    "\n",
    "        self.backbone = self._get_basemodel(base_model)\n",
    "        dim_mlp = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.backbone.fc1 = nn.Sequential(nn.Linear(dim_mlp, num_class))\n",
    "        self.backbone.fc2 = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), nn.Linear(dim_mlp, out_dim))\n",
    "    def _get_basemodel(self, model_name):\n",
    "        try:\n",
    "            model = self.resnet_dict[model_name]\n",
    "        except KeyError:\n",
    "            raise InvalidBackboneError(\n",
    "                \"Invalid backbone architecture. Check the config file and pass one of: resnet18 or resnet50\")\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone.fc1(self.backbone(x)),self.backbone.fc2(self.backbone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch SimCLR')\n",
    "parser.add_argument('-data', metavar='DIR', default='./datasets',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('-a', '--arch', metavar='ARCH', default='resnet50',\n",
    "                    choices=model_names,\n",
    "                    help='model architecture: ' +\n",
    "                         ' | '.join(model_names) +\n",
    "                         ' (default: resnet50)')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.00001, type=float,\n",
    "                    metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)',\n",
    "                    dest='weight_decay')\n",
    "parser.add_argument('--seed', default=None, type=int,\n",
    "                    help='seed for initializing training. ')\n",
    "parser.add_argument('--disable-cuda', action='store_true',\n",
    "                    help='Disable CUDA')\n",
    "parser.add_argument('--fp16-precision', action='store_true',\n",
    "                    help='Whether or not to use 16-bit precision GPU training.')\n",
    "\n",
    "parser.add_argument('--out_dim', default=128, type=int,\n",
    "                    help='feature dimension (default: 128)')\n",
    "parser.add_argument('--log-every-n-steps', default=100, type=int,\n",
    "                    help='Log every n steps')\n",
    "parser.add_argument('--gpu-index', default=0, type=int, help='Gpu index.')\n",
    "parser.add_argument('--instance_loss_alpha', default=0.7, type=float, help='Instance loss alpha.')\n",
    "parser.add_argument('--instance_loss_beta', default=0.3, type=float, help='Instance loss beta.')\n",
    "parser.add_argument('--instance_loss_gamma', default=0.3, type=float, help='Instance loss beta.')\n",
    "parser.add_argument('--num_class', default=4, type=int, help='number of class.')\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_known_args()\n",
    "    args = args[0]\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    with open(\"./instancelearning_Train.json\", 'r') as file:\n",
    "        json_Train = json.load(file)\n",
    "    with open(\"./instancelearning_Valid.json\", 'r') as file:\n",
    "        json_Valid = json.load(file)\n",
    "    with open(\"./instancelearning_Test.json\", 'r') as file:\n",
    "        json_Test = json.load(file)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "                                       transforms.ToTensor()\n",
    "                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "\n",
    "    valid_dataset = CustomDatset(all_json =  instance_data_sequence(json_Valid), transform = transform)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    train_dataset = CustomDatset(all_json = instance_data_sequence(json_Train), transform = transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    model = ResNetSimCLR(base_model=args.arch, out_dim=args.out_dim, num_class = args.num_class)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
    "                                                           last_epoch=-1)\n",
    "    with torch.cuda.device(args.gpu_index):\n",
    "        simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, args=args)\n",
    "        simclr.train(json_Train,valid_loader,transform)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa461ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4090",
   "language": "python",
   "name": "worms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
